{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwra7bslMnj06Hyz0GBDfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitTanmai/COVID-19/blob/master/resume%20filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq6L2oZMlLFa"
      },
      "source": [
        "#importing all required libraries\n",
        "\n",
        "import PyPDF2\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from spacy.matcher import PhraseMatcher"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLgOn-FHlihW"
      },
      "source": [
        "#Function to read resumes from the folder one by one\n",
        "mypath='/content/' #enter your path here where you saved the resumes\n",
        "onlyfiles = [os.path.join(mypath, f) for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC6hb3lxmZh9"
      },
      "source": [
        "def pdfextract(file):\n",
        "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
        "    countpage = fileReader.getNumPages()\n",
        "    count = 0\n",
        "    text = []\n",
        "    while count < countpage:    \n",
        "        pageObj = fileReader.getPage(count)\n",
        "        count +=1\n",
        "        t = pageObj.extractText()\n",
        "        print (t)\n",
        "        text.append(t)\n",
        "    return text\n",
        "\n",
        "#function to read resume ends"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEx1BSvYmjsn"
      },
      "source": [
        "#function that does phrase matching and builds a candidate profile\n",
        "def create_profile(file):\n",
        "    text = pdfextract(file) \n",
        "    text = str(text)\n",
        "    text = text.replace(\"\\\\n\", \"\")\n",
        "    text = text.lower()\n",
        "    #below is the csv where we have all the keywords, you can customize your own\n",
        "    keyword_dict = pd.read_csv('/content/Table1.csv')\n",
        "    stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
        "    NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
        "    ML_words = [nlp(text) for text in keyword_dict['Machine learning'].dropna(axis = 0)]\n",
        "    DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
        "    R_words = [nlp(text) for text in keyword_dict['R language'].dropna(axis = 0)]\n",
        "    python_words = [nlp(text) for text in keyword_dict['Python language'].dropna(axis = 0)]\n",
        "    Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
        "\n",
        "    matcher = PhraseMatcher(nlp.vocab)\n",
        "    matcher.add('Stats', None, *stats_words)\n",
        "    matcher.add('NLP', None, *NLP_words)\n",
        "    matcher.add('ML', None, *ML_words)\n",
        "    matcher.add('DL', None, *DL_words)\n",
        "    matcher.add('R', None, *R_words)\n",
        "    matcher.add('Python', None, *python_words)\n",
        "    matcher.add('DE', None, *Data_Engineering_words)\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    d = []  \n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
        "        span = doc[start : end]  # get the matched slice of the doc\n",
        "        d.append((rule_id, span.text))      \n",
        "    keywords = \"\\n\".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())\n",
        "    \n",
        "    ## convertimg string of keywords to dataframe\n",
        "    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])\n",
        "    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])\n",
        "    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])\n",
        "    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) \n",
        "    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(\")\"))\n",
        "    \n",
        "    base = os.path.basename(file)\n",
        "    filename = os.path.splitext(base)[0]\n",
        "       \n",
        "    name = filename.split('_')\n",
        "    name2 = name[0]\n",
        "    name2 = name2.lower()\n",
        "    ## converting str to dataframe\n",
        "    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])\n",
        "    \n",
        "    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)\n",
        "    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)\n",
        "\n",
        "    return(dataf)\n",
        "        \n",
        "#function ends"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUIyOEkUyl1X"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th9Ye7PBv30q",
        "outputId": "38dde36a-16ee-463b-f498-492931e1bd7e"
      },
      "source": [
        "#code to execute/call the above functions\n",
        "\n",
        "final_database=pd.DataFrame()\n",
        "i = 0 \n",
        "while i < len(onlyfiles):\n",
        "    file = onlyfiles[i]\n",
        "    if '.pdf' in file:\n",
        "      print('fffffffffffffffffffffffffffffffffffffffffffff',file)\n",
        "      dat = create_profile(file)\n",
        "      final_database = final_database.append(dat)\n",
        "      i +=1\n",
        "      #print(final_database)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fffffffffffffffffffffffffffffffffffffffffffff /content/2Resume.pdf\n",
            "April\n",
            " \n",
            "29\n",
            ", 202\n",
            "1\n",
            " \n",
            " \n",
            "                    \n",
            "RAVINDER SALUJA · RÉSUMÉ\n",
            " \n",
            "1\n",
            " \n",
            "Ravinder\n",
            " \n",
            "Salu\n",
            "j\n",
            "a\n",
            " \n",
            "D\n",
            "ATA \n",
            "S\n",
            "CIENTIST\n",
            " \n",
            "Mumbai Central\n",
            ", \n",
            "Mumbai\n",
            ", Maharashtra, India, Pin\n",
            " \n",
            "Code \n",
            "-\n",
            " \n",
            "400008.\n",
            " \n",
            " \n",
            "linkedin.com/in/ravindersaluja\n",
            "   \n",
            " \n",
            "|\n",
            " \n",
            " \n",
            "  \n",
            "  \n",
            "ravinder.saluja@yahoo.com\n",
            "  \n",
            " \n",
            " \n",
            "|\n",
            " \n",
            "  \n",
            " \n",
            " \n",
            " \n",
            "github.com/ravindersaluja\n",
            "  \n",
            " \n",
            " \n",
            "|\n",
            " \n",
            "  \n",
            " \n",
            " \n",
            "(\n",
            "+91\n",
            ")\n",
            " \n",
            "9870609759\n",
            " \n",
            "E\n",
            "xperience\n",
            " \n",
            "________________________________________________________________\n",
            "_______\n",
            " \n",
            "Tech Mahindra Business Services \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "              \n",
            "    \n",
            "Mumbai, India\n",
            " \n",
            "A\n",
            "NALYST\n",
            "                    \n",
            "                                                                                                          \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "May 2019 \n",
            "\n",
            " \n",
            "Present\n",
            " \n",
            "\n",
            " \n",
            "Building\n",
            " \n",
            "predictive \n",
            "models \n",
            "using Machine Learning \n",
            "a\n",
            "lgorithms \n",
            "for various \n",
            "business \n",
            "projects\n",
            ".\n",
            " \n",
            "\n",
            " \n",
            "End\n",
            "-\n",
            "to\n",
            "-\n",
            "end project management, \n",
            "communicating with \n",
            "stakeholders, \n",
            "understanding the business requirements\n",
            " \n",
            "and \n",
            "delivering \n",
            "projects \n",
            "accordingly.\n",
            " \n",
            "\n",
            " \n",
            "Propos\n",
            "ing\n",
            " \n",
            "solutions and strategies to business challenges and t\n",
            "aking suggestions \n",
            "from stakeholders for \n",
            "improving the \n",
            "developed\n",
            " \n",
            "models\n",
            ".\n",
            " \n",
            "\n",
            " \n",
            "Identifying valuable data sources \n",
            "for\n",
            " \n",
            "a\n",
            "utomating various manual \n",
            "tasks \n",
            "with the help of Python\n",
            " \n",
            "and delivering it as \n",
            "a\n",
            " \n",
            "Windows executable file\n",
            " \n",
            "/ scheduling the \n",
            "process with \n",
            "batch files\n",
            ".\n",
            " \n",
            "\n",
            " \n",
            "Preparing\n",
            " \n",
            "decks and dashboards for better visualization\n",
            " \n",
            "of the data, \n",
            "thus making derivation of insights easier.\n",
            " \n",
            "\n",
            " \n",
            "Leading a team\n",
            ", \n",
            "for scraping data from the web and building \n",
            "a dashboard\n",
            " \n",
            "for improving \n",
            "the overall customer experience.\n",
            " \n",
            "M\n",
            "ANAGEMENT \n",
            "T\n",
            "RAINEE\n",
            " \n",
            "                                                                               \n",
            " \n",
            "          \n",
            "Jan 2019 \n",
            "\n",
            " \n",
            "Apr 2019\n",
            " \n",
            "\n",
            " \n",
            "Worked \n",
            "on building \n",
            "a \n",
            "text \n",
            "classification model \n",
            "\n",
            " \n",
            "Naïve Bayes \n",
            "using \n",
            "N\n",
            "atural \n",
            "L\n",
            "anguage \n",
            "P\n",
            "rocessing\n",
            " \n",
            "techniques and the \n",
            "sklearn\n",
            " \n",
            "module \n",
            "for \n",
            "predicting the contact reason of the customer\n",
            " \n",
            "which helped with \n",
            "efficiently dealing with customers.\n",
            " \n",
            "\n",
            " \n",
            "Undertook preprocessing of structured and unstructured data.\n",
            " \n",
            "\n",
            " \n",
            "Built dashboards using D3.js for showcasing the key contact center KPIs in an interactive manne\n",
            "r.\n",
            " \n",
            " \n",
            "Edwise International LLP\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "  \n",
            "                \n",
            "Mumbai, India\n",
            " \n",
            " \n",
            "  \n",
            " \n",
            "F\n",
            "ACULTY\n",
            " \n",
            " \n",
            "       \n",
            "May\n",
            " \n",
            "2017 \n",
            "\n",
            " \n",
            "Jun\n",
            " \n",
            "2018\n",
            " \n",
            "\n",
            " \n",
            "Taught quantitative analysis to students appearing for international standardized tests like GRE, GMAT and SAT in group \n",
            "settings.\n",
            " \n",
            "\n",
            " \n",
            "Trained \n",
            "newly recruited faculty members for different branches.\n",
            " \n",
            "E\n",
            "ducation\n",
            " \n",
            "________________________________________________________________________\n",
            " \n",
            "S.P. Jain School of Global Management\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "      \n",
            "Mumbai, India\n",
            " \n",
            "Professional Certificate Program in Big Data and \n",
            "Analytics.\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "    \n",
            " \n",
            "    \n",
            " \n",
            "    \n",
            " \n",
            "         \n",
            " \n",
            "         \n",
            "Class of 2018\n",
            " \n",
            "Watumull Institute of Electronics Engineering and Computer Technology,\n",
            " \n",
            "University\n",
            " \n",
            "of \n",
            "Mumbai\n",
            " \n",
            " \n",
            "        \n",
            "Mumbai, India\n",
            " \n",
            "B.E. Electronics and Telecommunication.\n",
            " \n",
            "   \n",
            " \n",
            " \n",
            "    \n",
            "    \n",
            "      \n",
            "Class of 2016\n",
            " \n",
            "P\n",
            "rojects\n",
            " \n",
            "__________________________________________________________________________\n",
            " \n",
            "Role: \n",
            "Analyst, Tech Mahindra Business Services\n",
            " \n",
            "Current Project:\n",
            " \n",
            "M\n",
            "EDALLIA\n",
            "-\n",
            "B\n",
            "OLD\n",
            "C\n",
            "HAT \n",
            "T\n",
            "HEME \n",
            "A\n",
            "LIGNMENT\n",
            ", Python (pandas, \n",
            "nltk, \n",
            "sklearn \n",
            "-\n",
            " \n",
            "Naïve Bayes\n",
            ")\n",
            " \n",
            " \n",
            "\n",
            " \n",
            "The objective was to map \n",
            "the themes present on the Medallia\n",
            "\n",
            " \n",
            "portal \n",
            "across the survey verbatim data and align the same \n",
            "with \n",
            "\n",
            " \n",
            "survey data.\n",
            " \n",
            "The challenge was there were multiple themes \n",
            "assigned to surveys. So\n",
            ",\n",
            " \n",
            "this was a Multi\n",
            "-\n",
            "Label \n",
            "Multi\n",
            "-\n",
            "Class classification problem.\n",
            " \n",
            "\n",
            " \n",
            "The objective was to prepare \n",
            "multiple\n",
            " \n",
            "classification model\n",
            "s\n",
            " \n",
            "that would \n",
            "categorize\n",
            " \n",
            "the \n",
            "survey verbatim and \n",
            "those models \n",
            "trained, would \n",
            "help predict multiple themes for a single survey which would t\n",
            "hen be mapped against \n",
            "B\n",
            "old\n",
            "C\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "Approach: Clean the text data using various methods \n",
            "of \n",
            "the NLTK\n",
            "/\n",
            "textblob\n",
            " \n",
            "library\n",
            ", \n",
            "create custom methods for correcting \n",
            "spelling mistakes, preparing word list dictionary\n",
            " \n",
            "and n\n",
            "-\n",
            "grams, and finally\n",
            " \n",
            "creating a \n",
            "Document Term Matrix \n",
            "from the \n",
            "corpus with the help of \n",
            "TfidfVectorizer\n",
            " \n",
            "which was passed to the \n",
            "Naïve Bayes \n",
            "classification model.\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\n",
            "April\n",
            " \n",
            "29\n",
            ", 202\n",
            "1\n",
            " \n",
            " \n",
            "                    \n",
            "RAVINDER SALUJA · RÉSUMÉ\n",
            " \n",
            "2\n",
            " \n",
            "C\n",
            "HANGE \n",
            "O\n",
            "F \n",
            "T\n",
            "ARIFF \n",
            "(U\n",
            "PGRADES\n",
            ")\n",
            ", Python (\n",
            "pandas\n",
            ",\n",
            " \n",
            "sk\n",
            "l\n",
            "ea\n",
            "rn \n",
            "-\n",
            " \n",
            "kNN\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "The objective is to \n",
            "identify customers who are likely to upgrade their tariff by creating a \n",
            "predict\n",
            "ive model, trained on the \n",
            "customer \n",
            "demographics, usage\n",
            ",\n",
            " \n",
            "history of interactions \n",
            "and other derived attributes and also providing the probability rate \n",
            "and likelihood of the \n",
            "customer opting for upgrade.\n",
            " \n",
            "\n",
            " \n",
            "The customers thus identified will be converted through various campaigns which will help with the uplift in margin for \n",
            "the organization.\n",
            " \n",
            "\n",
            " \n",
            "Approach: \n",
            "Building \n",
            "project specific d\n",
            "ata \n",
            "m\n",
            "art\n",
            ", \n",
            "EDA\n",
            ", \n",
            "u\n",
            "nder\n",
            "-\n",
            "sampling data\n",
            ", \n",
            "developing a \n",
            "p\n",
            "ipeline with sklearn for \n",
            "preprocessing\n",
            "/data wrangling\n",
            " \n",
            "and model building \n",
            "along \n",
            "with cross\n",
            "-\n",
            "validation\n",
            ", model evaluation, tweaking\n",
            ", \n",
            "deploying\n",
            ".\n",
            " \n",
            "S\n",
            "PEECH\n",
            " \n",
            "A\n",
            "NALYTICS\n",
            ", \n",
            "P\n",
            "ython\n",
            " \n",
            "(\n",
            "pydub, \n",
            "ffmpeg, \n",
            "SpeechRecognition)\n",
            " \n",
            "\n",
            " \n",
            "The aim of this project \n",
            "was\n",
            " \n",
            "to transcribe the speech of the advisor and the customer into text and then further do the \n",
            "speaker diarization for the same.\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "chunks so as to pass it to \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "This help\n",
            "ed\n",
            " \n",
            "the Quality Monitoring Team \n",
            "in making the audit\n",
            " \n",
            "\n",
            "fo\n",
            "r the \n",
            "given \n",
            "task.\n",
            " \n",
            "S\n",
            "OCIAL \n",
            "M\n",
            "EDIA \n",
            "A\n",
            "NALYTICS\n",
            ", \n",
            "Python \n",
            "(\n",
            "p\n",
            "yquery\n",
            ", requests, BeautifulSoup\n",
            ", t\n",
            "extblob\n",
            ", TensorFlow, Transformers\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "Developed scrapers for social media websites like Twitter and Trustpilot for scraping \n",
            "the tweets and reviews about\n",
            " \n",
            "the \n",
            "client and the competitors\n",
            ", developed sentiments analysis modules for the same\n",
            " \n",
            "as well as \n",
            "W\n",
            "ord\n",
            " \n",
            "C\n",
            "loud\n",
            "s\n",
            " \n",
            "for better \n",
            "visualization. \n",
            " \n",
            "\n",
            " \n",
            "D\n",
            "eployed this on the AWS \n",
            "EC2 \n",
            "Instance\n",
            " \n",
            "so that the data could\n",
            " \n",
            "be fetched at a scheduled time and the dashboard hosted \n",
            "on the internal server can be updated accordingly.\n",
            " \n",
            "N\n",
            "EXT \n",
            "B\n",
            "EST \n",
            "A\n",
            "CTION\n",
            ", \n",
            "Python \n",
            "(\n",
            "pandas, nltk\n",
            ",\n",
            " \n",
            "textblob\n",
            ", \n",
            "sklearn \n",
            "-\n",
            " \n",
            "Naïve \n",
            "Bayes\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "The \n",
            "objective was to prepare \n",
            "a\n",
            "n\n",
            " \n",
            "NLP based\n",
            " \n",
            "classification model that would categorise the type of text \n",
            "data \n",
            "received on \n",
            "real time basis and give the advisor suggestions as to what next steps should be taken.\n",
            " \n",
            "\n",
            " \n",
            "Approach: Clean the text data using various methods of the NLTK library as well as create c\n",
            "ustom methods for correcting \n",
            "spelling mistakes, preparing word list dictionary\n",
            " \n",
            "and n\n",
            "-\n",
            "grams\n",
            ", and finally\n",
            " \n",
            "creating a \n",
            "Document Term Matrix \n",
            "from the \n",
            "corpus with the help of CountVectorizer \n",
            "which was passed to the \n",
            "Naïve Bayes \n",
            "classification model.\n",
            " \n",
            "UK\n",
            " \n",
            "M\n",
            "OBILE \n",
            "M\n",
            "ARKET\n",
            " \n",
            "W\n",
            "ATCHDOG\n",
            ", \n",
            "Python \n",
            "(s\n",
            "elenium\n",
            ", AWS \n",
            "EC2 \n",
            "Instance\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "Developed scrapers for various telecommunication websites in the UK with the help of selenium\n",
            " \n",
            "and extracted fields \n",
            "related to \n",
            "price plans which helped with the tariff comparison and \n",
            "deployed these scrapers on AWS EC2 Instance\n",
            " \n",
            "so that \n",
            "the data could\n",
            " \n",
            "be fetched at a scheduled time and the dashboard \n",
            "hosted on the internal server \n",
            "can be updated \n",
            "accordingly.\n",
            " \n",
            "Role: \n",
            "Management Trainee, Tech Mahindra Business\n",
            " \n",
            " \n",
            "C\n",
            "USTOMER \n",
            "P\n",
            "HONE \n",
            "B\n",
            "ILL\n",
            " \n",
            "-\n",
            " \n",
            "PDF\n",
            " \n",
            "S\n",
            "CRAPING\n",
            ", \n",
            "Python \n",
            "(\n",
            "PyPDF2, \n",
            "re\n",
            ", os\n",
            ", zipfile\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "The \n",
            "objective was to scrape the phone billing information of customer\n",
            "s from\n",
            " \n",
            "unstructured \n",
            "pdf file\n",
            "s\n",
            " \n",
            "stored in zip archives, \n",
            "transform it into a structured form \n",
            "and \n",
            "load\n",
            " \n",
            "the information thus scraped\n",
            " \n",
            "to the\n",
            " \n",
            "SQL \n",
            "Server \n",
            "database.\n",
            " \n",
            "A\n",
            "TTRITION \n",
            "P\n",
            "ROJECT \n",
            "ETL\n",
            ", \n",
            "Python \n",
            "(\n",
            "sqlalchemy\n",
            ", \n",
            "pyodbc\n",
            ", \n",
            "win32com\n",
            ")\n",
            " \n",
            "\n",
            " \n",
            "The aim of this project was to build an entire pipeline for \n",
            "automating the process of building a data mart for the Employee \n",
            "Attrition Project right from extracting the spreadsheets from Microsoft Outlook to transforming it into the required \n",
            "format and finally loading it to the SQL Server database.\n",
            " \n",
            "T\n",
            "ools\n",
            "/\n",
            "F\n",
            "r\n",
            "ameworks\n",
            " \n",
            "______________________________________________________________\n",
            "__\n",
            " \n",
            "Data Preprocessing\n",
            ": \n",
            "Pandas, Numpy.\n",
            " \n",
            "Machine/Deep Learning \n",
            "Implementation Frameworks\n",
            ": \n",
            "S\n",
            "cikit\n",
            "-\n",
            "Learn\n",
            ", \n",
            "K\n",
            "eras\n",
            ", \n",
            "TensorFlow\n",
            ".\n",
            " \n",
            "Natural Language Processing\n",
            ":\n",
            " \n",
            "NLTK\n",
            ", \n",
            "Textblob\n",
            ", \n",
            "Gensim.\n",
            " \n",
            "Development:\n",
            " \n",
            "S\n",
            "elenium\n",
            ", \n",
            "R\n",
            "equests, BeautifulSoup, \n",
            "Lackey, \n",
            "PyInstaller, \n",
            "M\n",
            "icrosoft\n",
            " \n",
            "SQL\n",
            " \n",
            "Server\n",
            ", Git\n",
            ".\n",
            " \n",
            "Data \n",
            "Visualization:\n",
            " \n",
            "Pandas\n",
            "-\n",
            "profiling, Tableau, \n",
            "D3.js\n",
            ", Matplotlib, Seaborn, Bokeh, Plotly\n",
            ".\n",
            " \n",
            "Cloud Services:\n",
            " \n",
            "AWS\n",
            " \n",
            "EC2\n",
            " \n",
            "Instance, Google \n",
            "c\n",
            "olab\n",
            "oratory\n",
            ", IBM \n",
            "Watson\n",
            " \n",
            "s\n",
            "tudio\n",
            ", Kaggle \n",
            "k\n",
            "ernel.\n",
            " \n",
            "Integrated Development Environments/Text Editors\n",
            ":\n",
            " \n",
            "Spyder, Jupyter Notebook, \n",
            "SQL Server Management Studio (SSMS), Sublime \n",
            "Text.\n",
            " \n",
            " \n",
            "\n",
            "April\n",
            " \n",
            "29\n",
            ", 202\n",
            "1\n",
            " \n",
            " \n",
            "                    \n",
            "RAVINDER SALUJA · RÉSUMÉ\n",
            " \n",
            "3\n",
            " \n",
            "C\n",
            "ommunity\n",
            " \n",
            "_______________________________________________________________________\n",
            " \n",
            "Organisation/Groups\n",
            " \n",
            "\n",
            " \n",
            "Member at Google Developers Group\n",
            ".\n",
            " \n",
            "\n",
            " \n",
            "Member at PyData \n",
            "\n",
            " \n",
            "Mumbai.\n",
            " \n",
            "\n",
            " \n",
            "Member at AIDL (Artificial Intelligence & Deep Learning)\n",
            " \n",
            "A\n",
            "dditional Certification\n",
            "s\n",
            " \n",
            "__________________________________________________________\n",
            " \n",
            "\n",
            " \n",
            "IBM AI Engineering Specialization \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "         \n",
            "Credentia\n",
            "l ID: 9FYWT34G9QJT\n",
            " \n",
            "\n",
            " \n",
            "Machine Learning with Python\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "HMZ5P46RKLXT\n",
            " \n",
            "\n",
            " \n",
            "Scalable Machine Learning on Big Data using Apache Spark\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "VFY97GAB56C6\n",
            " \n",
            "\n",
            " \n",
            "Introduction to Deep Learning and Neural Networks with Keras\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "DDUX7QERNGBM\n",
            " \n",
            "\n",
            " \n",
            "Deep Neural Networks with PyTorch \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "EPAF7H38KSET\n",
            " \n",
            "\n",
            " \n",
            "Building Deep Learning models with TensorFlow \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "3TXZKQLR6HMU\n",
            " \n",
            "\n",
            " \n",
            "AI Capstone Project with Deep Learning \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Credential ID: \n",
            "NSR88ECVB66A\n",
            " \n",
            "A\n",
            "chievements\n",
            " \n",
            "____________________________________________________________________\n",
            " \n",
            "\n",
            " \n",
            "Received \n",
            "the \n",
            "\n",
            "Ace of Spades\n",
            "\n",
            " \n",
            "Award \n",
            "as an Analyst\n",
            " \n",
            "from Tech Mahindra Business Services\n",
            " \n",
            "for\n",
            " \n",
            "delivering quality projects using \n",
            "Machine Learning Algorithms\n",
            " \n",
            "for Q4 2020\n",
            ".\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "and\n",
            " \n",
            "project delivery \n",
            "as a Management Trainee at Tech Mahindra Business Services.\n",
            " \n",
            "L\n",
            "anguages\n",
            " \n",
            "_______________________________________________________________________\n",
            "_\n",
            " \n",
            "English\n",
            ",\n",
            " \n",
            "Hindi.\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}